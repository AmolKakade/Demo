Spark SQL
------------------------------------------------------------------------------------------------------------------------------------------------------
- Provides a SQL support for structured and semi-structured data in optimized way.
— Spark SQL offers querying and persisting structured and semi-structured data using structured queries.
- Spark SQL job end up as a tree of Catalyst expressions with further optimizations along the way to your large distributed data sets.
- Spark SQL offers performance query optimizations using
    1. Catalyst’s Logical Query Plan Optimizer
    2. Code generation (that could often be better than your own custom handmade code!) 
    3. Tungsten execution engine - It has its own Internal, In-memory, Binary Row Format which uses sun.misc.Unsafe.
 
 Query Plan
 - When we type, df.explain(), we usually see four plans as provided by Spark:
    Unresolved Logical Plan
    Resolved Logical Plan / Analysed Plan / Logical Plan
    Optimized Logical Plan
    Physical Plan
- These four plans are generated over three phases by Spark’s optimization engine - Catalyst Optimizer. 
- The Catalyst optimizer provides both rule-based (using heuristics) as well as cost-based optimizations.
- Spark transforms SQL query like
  
  SQL -> Unresolved Logical Plan -> Analysis -> Logical Plan -> Optimized Logical Plan -> Physical Plan -> Selected Physical Plan( with Cost Model applied) -> RDD(with Codegen)

Analysis
- Optimizer cross-checks Unresolved Logical Plan with the Catalog, to verify if the query is correct.
- Analyzer tries to identify the data type, existence, and location of the columns specified by the query. 
- The Analyzer also validates if the operation on each column is applicable based on the data type.
- If the query resolves successfully, anlyzer add  additional information like the data type of the column, the table the column belongs to, etc to Query Plan.

Logic Optimization
- Catalyst optimizer optimizes the query using rule-based optimization to derive an Optimized Logical Plan.

Physical planning
- Optimizer takes the optimized logical plan and generates one or more physical plans. 
- During this phase, Spark decides on which algorithm must be used for every operator.
- For example, the decision on which join algorithm must be used, whether SortMergeJoin or BroadcastHashJoin, etc, is made at this stage.
- The best plan is then selected using a cost-based model (comparison between model costs).

Code generation
- Finally, once the best physical plan is chosen for a SQL, Spark uses Tungsten Backend to generate Java bytecode.


Tungsten Execution Backend
------------------------------------------------------------------------------------------------------------------------------------------------------
- The goal of Project Tungsten is to improve Spark execution by optimizing Spark jobs for CPU and MEMORY EFFICIENCY.

E.g. **********
      // million integers
      scala> val intsMM = 1 to math.pow(10, 6).toInt

      // that gives ca 3.8 MB in memory
      scala> sc.parallelize(intsMM).cache.count
      res0: Long = 1000000

      // that gives ca 998.4 KB in memory
      scala> intsMM.toDF.cache.count                      // SEE the size. Dataframe and SparkSQL uses Tungsten as compared to RDD
      res1: Long = 1000000

- Tungsten optimization features:
    1. Off-Heap Memory Management
      - Uses binary, in-memory data representation aka Tungsten row format by using sun.misc.Unsafe instead of JVM.
****  - Uses its own OWN off-heap binary memory management which improves a JVM garbage collection alotinstead of JVM objects.    

    2. Cache Locality
    - Tungsten uses algorithms and cache-aware data structures that exploit the physical machine caches at different levels - L1, L2, L3.

    3. Whole-Stage Code Generation (aka CodeGen)
    - Whole-Stage Code Generation is enabled by default.
    - It improves the execution performance of a query by collapsing a query tree into a single optimized function.
    - It eliminates virtual function calls and leverages CPU registers for intermediate data.
    - Tungsten does CODE GENERATION AT COMPILE TIME and generates JVM bytecode to access Tungsten-managed memory structures that gives a very fast access.

**** 
Joins in SparkSQL ( Ref link: https://medium.com/@achilleus/https-medium-com-joins-in-apache-spark-part-3-1d40c1e51e1c AND 
https://medium.com/@achilleus/https-medium-com-joins-in-apache-spark-part-2-5b038bc7455b  )
------------------------------------------------------------------------------------------------------------------------------------------------------
1. BroadcastHashJoin (BHJ): 
      - Usually used when the data on one side of the join is very small (a few MBs). 
      - The smaller table is broadcasted to every executor (Exchange), and then joined with the bigger table, using a Hash Join (Join)
      - NO SHUFFLING involved in this and can be much quicker.
****  - To enforce BroadcastHashJoin we can use the broadcast HINT.
          
          scala> customer.join(broadcast(payment),“customerId”)                // Here we used hint broadcast()
      
      spark.sql.autoBroadcastJoinThreshold - This can be configured to set the Maximum size in bytes for a dataframe to be broadcasted
        E.g.
        scala> spark.conf.get(“spark.sql.autoBroadcastJoinThreshold”)
        res0: String = 10485760
            - (-1) will disable broadcast join
             - Default is 10485760 ie 10MB
          
      spark.broadcast.compress - Used to configure whether to compress the data before sending it. 
          - It uses the compression specified in the spark.io.compression.codec config and the default is lz4. 
          - We can use other compression codecs such as lz4,lzf, snappy, ZStandard.
          - spark.broadcast.compress is TRUE BY DEFAULT.
       
      customer.join(payment,“customerId”).queryExecution.executedPlan        // Here we NO hint is used so no guarantee of BroadcastHashJoin
      
NOTE:
- If a broadcast hint is specified, the join side with the hint will be broadcasted irrespective of autoBroadcastJoinThreshold. 
- If BOTH SIDES of the join have the broadcast HINTS, the side with a smaller estimated physical size will be broadcasted. 
- If there is NO HINT and if the estimated physical size of the table < autoBroadcastJoinThreshold then that table is broadcasted to all the executor nodes.  

2. ShuffledHashJoin (SHJ): 
      - Used when the data on one side of the join is at least 3 times smaller than side AND the average partition size is small enough to broadcast. 
      - During this join, the PARTITIONS are broadcasted (Exchange) and joined using Hash Join (Join).
      - Shuffle hash join breaks 1 big join of 2 tables into smaller chunks of the localized relatively smaller branch. 
      - It has 2 phases
          1. Shuffle:
              - Shuffles data across the partitions from both tables/dataframes. 
              - If 2 tables have the same keys, they end up in the same partition so that the data required for joins is available in the same partition.
          2. Hash Join: 
              - The data on each partition performs a classic single node hash join algorithm based on SHUFFLED partitions in first phase.

NOTE:
- Performance of this is based on the distribution of keys in the dataset.
- Prefer EmpID as join key over DeptID as EmpID will have more values, so more parallelism.
- This is not well suited for joins that wouldn’t fit in memory.
- By default, SortMergeJoin is preffered over ShuffledHashJoin.
- Shuffle Hash join will be used as the join strategy only when;
      spark.sql.join.preferSortMergeJoin is set to FALSE
      AND the cost to build a hash map is less than sorting the data. 

 3. SortMergeJoin (SMJ): 
  - SortMergeJoin is the default join strategy if the matching join keys are sortable and not eligible for broadcast join or shuffle hash join.
  - This happens when we cannot apply either of the above two algorithms. 
  - During this join, the data on both sides of the join are sorted (2 Sort steps) and then joined using MergeSort (Join).
  - SCALABLE: What makes it scalable is that it can spill the data to the disk and doesn’t require the entire data to fit inside the memory.
  - It has 3 phases:
      Shuffle Phase: The 2 large tables are REPARTIONED as per the join keys across the partitions in the cluster.
      Sort Phase: SORT the data within each partition parallelly.
      Merge Phase: Join the 2 SORTED + REPARTIONED data. 

NOTE: 
- Performance-wise BHJ is better than SHJ which is better than SMJ
- Sometimes spark by default chooses Merge Sort Join which might not be always ideal.
- SortMergeJoin is more scalable because of spilling 

4. CrossJoin/CartesianJoin (SMJ): 
  - This type of join is NOT RECOMMENEDED UNTIL NECESSARY.
  - Join of table-1(m rows) with table-2(n rows) results total(m * n) rows
  - This join will be triggered when no join key is specified
  
  E.g.
      scala> customer.join(payment)                                         // NO JOIN KEY SPECIFIED *****
      scala> customer.join(payment).show
        org.apache.spark.sql.AnalysisException: Detected implicit cartesian product for INNER join between logical plans
        Project [_1#17 AS customerId#20, _2#18 AS name#21]
        ....
        Join condition is missing or trivial.                // SPARK Gives AnalysisException just to inform user regarding accidental usage of CrossJoin
**** - So to use CrossJoin, we have to set spark.sql.crossJoin.enabled=true

5. BroadcastNestedLoopJoin
- This type of join is chosen when NO JOINING KEYS are specified AND either has a broadcast hint 
OR if 1 side of the join could be broadcasted and is less than spark.sql.autoBroadcastJoinThreshold.
- This type of join is ALSO the final fall back option if no join keys are specified, and is not an inner join. 
- This could be very slow and can cause OutOfMemoryExceptions!

6. LeftSemi Join
  - Returns only the COLUMNS from the left side that has a match on the right side based on the condition provided for the join statement.
  - This can also be achieved in subquery kind of queries in conjunction with IN/EXISTS in SQL.
  - Advantage of LeftSemiis it restricts the amount of data that is read from the right side table.
  
  E.g.
scala> payment.show
+---------+----------+------+
|paymentId|customerId|amount|
+---------+----------+------+
|        1|       101|  2500|
|        2|       102|  1110|
|        3|       103|   500|
|        4|       104|   400|
|        5|       105|   150|
|        6|       106|   450|
+---------+----------+------+
scala> customer.show
+----------+----+
|customerId|name|
+----------+----+
|       101| Jon|
|       102|Aron|
|       103| Sam|
+----------+----+
scala> payment.join(customer, payment("customerId") === customer("customerId"), "leftsemi").show             // leftsemi is used
+---------+----------+------+
|paymentId|customerId|amount|
+---------+----------+------+
|        1|       101|  2500|
|        3|       103|   500|
|        2|       102|  1110|
+---------+----------+------+

NOTE:
- Joined output only consists of data from the Payment(Left) table which has a match for it in the Customer(Right) table. 
- "name" column from the Customer table is not returned even for the matching customerId.
 
7. LeftAnti-Join 
- Does exactly reverse as of LeftSemiJoin in terms of ROWS selection.
- IT ALSO ONLY GIVES columns from left table.
- The output would just return the data from LEFT TABLE that DOESN'T HAVE MATCH ON RIGHT side table.

 scala> payment.join(customer, payment("customerId") === customer("customerId"), "leftanti").show()       // leftanti is used
+---------+----------+------+
|paymentId|customerId|amount|
+---------+----------+------+
|        5|       105|   150|
|        6|       106|   450|
|        4|       104|   400|
+---------+----------+------+

8. SelfJoin
- In self join, we join the dataframe with itself. 
- USE OF ALIASING is must to avoid column name collisions.

E.g.
val employee1 = spark.createDataFrame(Seq(
  (1,"ceo",None),
  (2,"manager1",Some(1)),
  (3,"manager2",Some(1)),
  (101,"Amy",Some(2)),
  (102,"Sam",Some(2)),
  (103,"Aron",Some(3)),
  (104,"Bobby",Some(3)),
  (105,"Jon", Some(3))
)).toDF("employeeId","employeeName","managerId")

scala> val selfJoinedEmp = employee1.as("e").join(employee1.as("m"),$"m.employeeId" === $"e.managerId")   // SelfJoin is NOT USED *********


Joins on columns with nulls ***********
- If we wanted to join on columns which have NULLS in it. By default, Spark would skip these columns.
- Say I want to join, df1 and df2 on id column which has nulls in it. The result would not have the null values as shown below.

scala> df1.show              scala> df2.show
+----+-----+                 +----+-----+
|  id| name|                 |  id| dept|
+----+-----+                 +----+-----+
| 123|name1|                 |null|sales|
| 456|name3|                 | 223|Legal|
|null|name2|                 | 456|   IT|
+----+-----+                 +----+-----+

scala> df1.join(df2, Seq("id")).show
+---+-----+-----+
| id| name| dept|
+---+-----+-----+
|123|name1|sales|
|456|name3|   IT|
+---+-----+-----+

- We have to use Equality Test Operator(<=>) instead of ===.
scala> df1.join(df2,df1("id") <=> df2("id")).drop(df1("id")).show
+-----+----+-----+
| name|  id| dept|
+-----+----+-----+
|name1| 123|sales|
|name3|    |   IT|
|name2|null|sales|
+-----+----+-----+


Spark catalog
- Catalog is the interface for managing a metastore (aka metadata catalog) of relational entities.
- Manages relational entities like - database, tables, functions, table columns and temporary views etc.
- Catalog is available using SparkSession.catalog property.
E.g.
        scala> :type spark
        org.apache.spark.sql.SparkSession

        scala> :type spark.catalog
        org.apache.spark.sql.catalog.Catalog
- Mostly used methids are;
cacheTable() - cache's table
clearCache() - clears the cached table from memory
createTable()
currentDatabase()
databaseExists()


TempView vs GlobalTempView
- TempView- Temporary views in Spark SQL are session-scoped and will disappear if the session that creates it terminates.
- GlobalTempView - These are shared among all sessions and keep alive until the Spark application terminates.
- Global temporary view is tied to a system preserved database global_temp, and we must use the qualified name to refer it.

df.createGlobalTempView("people")


Types of functions in Spark
1. Standard Functions — functions object
- org.apache.spark.sql.functions object offers many built-in functions to process values in Columns in Datasets.
        
scala> import org.apache.spark.sql.functions._

scala> Seq(Array(0,1,2)).toDF("array").withColumn("num", explode('array)).show              //explode()
+---------+---+
|    array|num|
+---------+---+
|[0, 1, 2]|  0|
|[0, 1, 2]|  1|
|[0, 1, 2]|  2|
+---------+---+

*******
User Defined functions ( ref link - https://docs.databricks.com/spark/latest/spark-sql/udf-scala.html)
- User defined function operate on COLUMNS and we can use them both with DataFrame API as well as Spark SQL
- Define, register and use with Spark SQL
    1. Define and Register UDF
        
        scala> val squared = (s: Long) => {
            s * s
         }

        scala> spark.udf.register("square", squared)                                    // Register as square() in Spark SQL.
        
    2. Use it Spark SQL
        scala> spark.range(1, 20).createOrReplaceTempView("test")
        scala> spark.sql(select id, square(id) as id_squared from test)

- Define and Use of UDF with Dataframe API
       
        scala> import org.apache.spark.sql.functions.{col, udf}
        scala> val squared = udf((s: Long) => s * s)
        scala> spark.range(1, 20).select(squared(col("id")) as "id_squared").show()

2. Aggregation functions
- You can group records in a Dataset using a condition to compute aggregates (over a collection of grouped records).
E.g.-1
scala> spark.range(10).agg(sum('id) as "sum").show                           // Aggregation without group
+---+
|sum|
+---+
| 45|
+---+

E.g.-2
scala> val ints = 1 to math.pow(10, 3).toInt
scala> val dataset = ints.toDF("n").withColumn("m", 'n % 2)                  
scala> dataset.groupBy('m).agg(sum('n)).show                                        // Aggregation with group. sum()
+---+------+
|  m|sum(n)|
+---+------+
|  1|250000|
|  0|250500|
+---+------+

E.g.-3
scala> dataset.show
+----+---------+-----+
|name|productId|score|
+----+---------+-----+
| aaa|      100| 0.12|
| aaa|      200| 0.29|
| bbb|      200| 0.53|
| bbb|      300| 0.42|
+----+---------+-----+

scala> dataset.groupBy('name).avg().show                                            // Aggregation with group. avg()
+----+--------------+----------+
|name|avg(productId)|avg(score)|
+----+--------------+----------+
| aaa|         150.0|     0.205|
| bbb|         250.0|     0.475|
+----+--------------+----------+

scala> dataset.groupBy('name, 'productId).agg(Map("score" -> "avg")).show           // Aggregation with group. avg()
+----+---------+----------+
|name|productId|avg(score)|
+----+---------+----------+
| aaa|      200|      0.29|
| bbb|      200|      0.53|
| bbb|      300|      0.42|
| aaa|      100|      0.12|
+----+---------+----------+

scala> dataset.groupBy('name).count.show                                            // Aggregation with group. count()
+----+-----+
|name|count|
+----+-----+
| aaa|    2|
| bbb|    2|
+----+-----+

scala> dataset.groupBy('name).max("score").show                                     // Aggregation with group. max()
+----+----------+
|name|max(score)|
+----+----------+
| aaa|      0.29|
| bbb|      0.53|
+----+----------+

scala> dataset.groupBy('name).sum("score").show                                     // Aggregation with group. sum()
+----+----------+
|name|sum(score)|
+----+----------+
| aaa|      0.41|
| bbb|      0.95|
+----+----------+


3. Windowed Aggregates
- Window functions perform a calculation on a group of records (called a window) that are in some relation to the current record. 
- They calculate values for every records in a window.
- Windowed functions contain list of ranking function like : rank, dense_rank, row_number etc.

E.g. - 1
case class Salary(depName: String, empNo: Long, salary: Long)
val empsalary = Seq(
  Salary("sales", 1, 5000),
  Salary("personnel", 2, 3900),
  Salary("sales", 3, 4800),
  Salary("sales", 4, 4800),
  Salary("personnel", 5, 3500),
  Salary("develop", 7, 4200),
  Salary("develop", 8, 6000),
  Salary("develop", 9, 4500),
  Salary("develop", 10, 5200),
  Salary("develop", 11, 5200)).toDS

import org.apache.spark.sql.expressions.Window
scala> val byDepName = Window.partitionBy('depName)                                 // Windows are partitions of deptName
scala> empsalary.withColumn("avg", avg('salary) over byDepName).show
+---------+-----+------+-----------------+
|  depName|empNo|salary|              avg|
+---------+-----+------+-----------------+
|  develop|    7|  4200|           5020.0|
|  develop|    8|  6000|           5020.0|
|  develop|    9|  4500|           5020.0|
|  develop|   10|  5200|           5020.0|
|  develop|   11|  5200|           5020.0|
|    sales|    1|  5000|4866.666666666667|
|    sales|    3|  4800|4866.666666666667|
|    sales|    4|  4800|4866.666666666667|
|personnel|    2|  3900|           3700.0|
|personnel|    5|  3500|           3700.0|
+---------+-----+------+-----------------+

E.g.-2
val dataset = spark.range(9).withColumn("bucket", 'id % 3)

import org.apache.spark.sql.expressions.Window
val byBucket = Window.partitionBy('bucket).orderBy('id)

scala> dataset.withColumn("rank", rank over byBucket).show                          // calling rank()
+---+------+----+
| id|bucket|rank|
+---+------+----+
|  0|     0|   1|
|  3|     0|   2|
|  6|     0|   3|
|  1|     1|   1|
|  4|     1|   2|
|  7|     1|   3|
|  2|     2|   1|
|  5|     2|   2|
|  8|     2|   3|
+---+------+----+


Data Skewness in Spark Partitions
- Joining on a key that is not evenly distributed across the cluster, causing SOME PARTITIONS TO BE LONG and not allowing Spark to process data in parallel.
Solutions:
1. BroadcastHashJoin
    - Increase Threshold with - spark.sql.autoBroadcastJoinThreshold.
    - Allocate more memory to Executors and Driver( to accomodate Broadcast)
    - If one of the table in join is small enowugh to fit in memory(both of Driver and Executor) go with BroadcastHashJoin.
    - E.g. If small table size if 1 GB and we have 100 executors, then total memory consumption after broadcast is - 100 GB ( For executors) + 1 GB (Driver)

val df = spark.read.parquet(“s3://...”)
val geoDataDf = spark.read.parquet(“s3://...”)
val userAgentDf = spark.read.parquet(“s3://...”)
val ownerMetadataDf = spark.read.parquet(“s3://...”)df
 .join(broadcast(geoDataDf), exprGeo, “left”)                                               // Using Broadcast hint
 .join(broadcast(userAgentDf), exprUserAgent, “left”)                                       // Using Broadcast hint
 .join(broadcast(ownerMetadataDf), exprOwnerMetadata, “left”)                               // Using Broadcast hint
 .write
 .parquet(“s3://...”)

****
2. Salting ( Ref link: https://dzone.com/articles/why-your-spark-apps-are-slow-or-failing-part-ii-da)
- If the join key distribution is not evenly distributed salting will be useful.
- In case of salting WE GENERATE kind of Program generated keys called as SALT_KEY.
        Steps;
        - for big table ADD SOME random values to the join key( name it as SALTED_KEY)
        - In smaller table we need to REPLICATE the rows to match the random keys
        - The idea is if the join condition is satisfied by key1 == key1, it should also get satisfied by key1_<salt> = key1_<salt>. 
        - The value of salt will help the dataset to be more evenly distributed.

Spark Optimization
- Choose Dataframe/ Spark SQL oevr RDD whereever possible because of Tungsten Engine and Query Optimization.
- If working with joins, check if join keys are evenely distributed if not go with SALTING
- If working with joins, if one of table small use BroadcastHashJoin
- Correct file format : Choose correct file format as per needs. Choose Columnar over Text, JSON etc.
- Chhose specific columns only instead of select *. Select * in case of Columnar format triggeres Entire table scan.
- Choose correct number of executors, executor memory, driver memory. Sometime more number of executors work fine for SCATTERED data( Peak data load for 2018)
- Use specific Queus to submit application instead of Default.
- Garbage collection ( Ref link: https://dzone.com/articles/why-your-spark-apps-are-slow-or-failing-part-ii-da)
    - It can be observed on Spark UI, Under executor tab.
    - Choose optimized data types - Array over List
    - Built-in vs. User Defined Functions (UDFs) - Built-in functions are more optimized
    - Avoid creating unwanted objects on JVM

Driver, Executor, Various Contexts, architecture, Spark Master
RDD, DataSet
SparkSession
Stages, Job, Tasks
DAG, DAG Scheduler
Encoders
SparkConf
Block Manager
Product types?
what is the need of directed acyclic graph in Spark, how to create DAG in Spark and how it helps in achieving fault tolerance.
difference between Apache Spark and Hadoop MapReduce

ref links: 
https://mallikarjuna_g.gitbooks.io/spark/content/spark-dagscheduler.html
https://luminousmen.com/post/spark-anatomy-of-spark-application
https://spark.apache.org/docs/latest/cluster-overview.html

RDD ( Resilient Distributed Dataset)
- RDD is basic abstraction in Spark. It represents an immutable, distributed, partitioned collection of elements that can be operated on in parallel.
- Meaning of RDD:
    Resilient - It is fault-tolerant with the help of RDD lineage graph and so able to recompute missing or damaged partitions due to node failures.
    Distributed -  Data residing on multiple nodes in a cluster.
    Dataset - Collection of partitioned data
- RDD has the following additional traits:
    In-Memory  - data inside RDD is stored in memory as much SIZE and long TIME as possible.
    Immutable  - It does not change once created and can only be transformed using transformations to new RDDs.
    Lazy evaluated  - Data inside RDD is not available or transformed until an ACTION is executed that triggers the execution.
    Cacheable  - You can hold all the data in a persistent storage like memory (default and the most preferred) or disk (the least preferred due to access speed).
    Parallel  - Process data in parallel.
    Typed. - Values in a RDD have types, e.g. RDD[Long] or RDD[(Int, String)].
    Partitioned. -  Data inside a RDD is partitioned (split into partitions) and then distributed across nodes in a cluster (one partition per JVM that may or may not correspond to a single node).
- RDDs support two kinds of operations:
  1. transformations
    - lazy operations(map, flatMap, filter, reduceByKey, join, cogroup, etc.) that return another RDD.
    - There are two kinds of transformations: narrow transformation, wide transformation.
        1. Narrow Transformations:
            - Common Narrow transformations are map, flatMap, MapPartitions, filter, sample & union
            - In narrow transformations the input & output data stays in the same partition, i.e. it is self-sufficient and SHUFFLING won't happen.
            - EACH partition in OUTPUT RDD has partitions with records that originate from a SINGLE partition in the parent RDD.
        2. Wide Transformation:
            - Wide transformations are also known as SHUFFLE TRANSFORMATIONS because data shuffle is required to process the data.
            - Common wide transformation are — gropupByKey, ReduceByKey, coalesce, repartition, join, intersection
            - The data required to compute the records in may live in many partitions of the parent RDD.
  2. actions 
    - Actions are RDD operations that produce non-RDD values. 
    - Simply actions evaluates the RDD lineage graph.
    - Actions TRIGGERS jobs using SparkContext.runJob or directly DAGScheduler.runJob.
    - They materialize a value in a Spark program. 
    - Actions are synchronous.
****    - Actions are one of two ways to SEND DATA FROM Executor TO Driver (the other being accumulators).
    - Few Actions are;
          aggregate(), collect(), count(), first(), fold(), foreach(), foreachPartition(), max(), min(), reduce(), saveAs* actions (e.g. saveAsTextFile, saveAsHadoopFile etc),
          take(), takeOrdered(), top() etc
          
**** 
- Intrinsic properties of the RDD: ( ref link https://medium.com/@gkadam2011/beneath-rdd-resilient-distributed-dataset-in-apache-spark-260c0b7250c6)
            abstract class RDD[T] {
                def compute(split: Partition, context: TaskContext): Iterator[T]
                def getPartitions: Array[Partition]
                def getDependencies: Seq[Dependency[_]]
                def getPreferredLocations(split: Partition): Seq[String] = Nil
                val partitioner: Option[Partitioner] = None
                }
                
    getDependencies() - List of parent RDD’s that are the dependencies of the RDD.
    An array of partitions that a dataset is divided to.
    compute() - A compute function to do a computation on partitions.
    An optional Partitioner that defines how keys are hashed, and the pairs partitioned (for key-value RDDs)
    Optional locality info, i.e. hosts for a partition where the records live or are the closest to read from.

Types of RDDs
- There are some of the most interesting types of RDDs:
    ParallelCollectionRDD - sc.parallelize(1 ro 10, 2)
    CoGroupedRDD
    HadoopRDD  - RDD that provides core functionality for reading data stored in HDFS using the older MapReduce API e.g. use case is the return RDD of SparkContext.textFile.
    MapPartitionsRDD — a result of calling operations like map, flatMap, filter, mapPartitions, etc.
    CoalescedRDD — a result of repartition or coalesce transformations.
    ShuffledRDD — a result of shuffling, e.g. after repartition or coalesce transformations.
    PipedRDD — an RDD created by piping elements to a forked external process.
    PairRDD (implicit conversion by PairRDDFunctions) that is an RDD of key-value pairs that is a result of groupByKey and join operations.
    DoubleRDD (implicit conversion as org.apache.spark.rdd.DoubleRDDFunctions) that is an RDD of Double type.
    SequenceFileRDD (implicit conversion as org.apache.spark.rdd.SequenceFileRDDFunctions) that is an RDD that can be saved as a SequenceFile.
- Appropriate operations of a given RDD type are automatically available on a RDD of the right type, e.g. RDD[(Int, Int)], through implicit conversion in Scala.

Limitations of RDD
- Not optimized — When working with structured data, RDDs cannot take advantages of Spark’s advanced optimizers including catalyst optimizer and Tungsten execution engine. Developers need to optimize each RDD based on its attributes.
- Schema Infer — Unlike Dataframe and datasets, RDDs DON'T INFER the schema of the ingested data and requires the user to specify it.
- Slow Performance — Being in-memory JVM objects, RDDs involve the overhead of Garbage Collection and Java Serialization which are expensive when data grows.
- Slow for non-JVM languages like python


Ways to Create RDD
1. Using parallelized collection like below;
 E.g.
        val data = Array(1, 2, 3, 4, 5)
        sc.paralleleize(data, 2)
        sc.makeRDD(data)
        sc.range(1 to 100 by 2, 5)
2. From external datasets:
- Spark RDD’s can be created from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3 etc.
- Text file RDDs can be created using sc.textFile(name, partitions)method.

3. From existing apache spark RDDs.
- By using transformation on existing RDDs.


Partition in RDD( number of partitions/tasks/relationship between them)
- A partition (aka split) is a logical chunk of a large distributed data set.
*********** - By default number of partitions = number of available blocks HDFS ( if reading data from HDFS )
*********** - By default number of partitions = number of available cores on machine ( example below )
- Data Locality - Spark tries to read data into an RDD from the nodes that are close to it.

E.g. 

scala> val df = sc.parallelize(1 to 100).count        // This code line on Spark UI shows 8 tasks are launched 
res0: Long = 100

scala> df.partitions.size                             // Gives number of partitions as 8

ints = val df = sc.parallelize(1 to 100 , 2).count.   // In this case we specified number of partitions to be 2

******** V.IMP******
NOTE: 
- The reason for 8 Tasks in Total is that machine is 8-core laptop and FEW partitions could be empty. 
- By default the number of partitions = number of all available cores.
- The maximum size of a partition = available memory of an executor.
- Spark can only run 1 concurrent task, for every partition of an RDD, up to the number of cores in your cluster. 
- So if you have a cluster with 50 cores, you want your RDDs to at least have 50 partitions (and probably 2-3x times that).
- In the first RDD transformation, e.g. reading from a file using sc.textFile(path, partition), the partition parameter will be applied to all further transformations and actions on this RDD.

COMPRESSED file read from HDFS 
- For Compressed files (file.txt.gz not file.txt or similar) Spark don't do split by default.
- When using textFile() with compressed files, Spark makes an RDD with only 1 partition.
- To do so we have to use repartition() specifically as given below.

repartition() Transformation
- With the computation repartition(5) causes 5 tasks to be started using NODE_LOCAL data locality.
- It does SHUFFLING

coalesce() Transformation
- coalesce(numPartitions: Int, shuffle: Boolean = false)
- The coalesce() transformation is used to change the number of partitions. 
***- It can trigger RDD shuffling depending on the shuffle flag (DISABLED by default, i.e. false).
- We can use toDebugString() to see lineage gragh.

HashPartitioner ( org.apache.spark - Class HashPartitioner)
- It us subclass of org.apache.spark.Partitioner
- HashPartitioner partitions RDD to configurable number of partitions to shuffle data around.
- Methods of HashPartitioner
    numPartitions() - Partitions data into exactly number number of partitions specified
    getPartition()  - Returns number of partitions

RDD shuffling
- Shuffling is the process of data transfer between stages.
- Shuffling does redistribution of data across partitions (aka repartitioning).
- It may or may not cause moving data across JVM processes.

***** - Avoid shuffling at all cost. Think about ways to leverage existing partitions. Leverage partial aggregation to reduce data transfer. 

NOTE:
- Shuffling doesn’t change the number of partitions, but change their content.
- Avoid using groupByKey(). Instead use reduceByKey() or combineByKey().
    groupByKey() - shuffles all the data, which is slow.
    reduceByKey() - shuffles only the results of sub-aggregations in each partition of the data.

Driver
- It is the process running the main() function of the application and creating the SparkContext.
- The Driver is responsible for converting a user application to smaller execution units called TASKS and then SCHEDULES them to run with a cluster manager on executors.
- The Driver is also responsible for executing the Spark application and returning the status/results to the user.
- Spark Driver contains various components 
  – DAGScheduler
  - TaskScheduler
  - BackendScheduler
  - BlockManager

- Driver properties:
  - Runs as a seperate process in its own JVM.
  - stores metadata about all Resilient Distributed Databases and their partitions;
  - is created after the user sends the Spark application to the cluster manager (YARN in our case);
  - optimizes logical DAG transformations and, if possible, combines them in stages and determines the best location for execution of this DAG;
  - creates Spark WebUI with detailed information about the application;
  

SparkContext ( org.apache.spark -  class SparkContext extends Logging )
- Entry point for Spark application.
- SparkContext can connect to several types of cluster managers ( Spark standalone cluster manager, Mesos, YARN, Kubernetes), which allocate resources across applications.
- A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster. 
- Spark Context also tracks executors in real-time by sending regular heartbeat messages.
- Spark Context stops working after the Spark application is finished.

 SparkSession 
- SparkSession is the entry point to Spark SQL. 
- It is the very first object you have to create to start developing Spark SQL applications.
- You use the SparkSession.builder method to create an instance of SparkSession.
 **** - SparkSession has merged SQLContext and HiveContext in one object as of Spark 2.0.0. 
     
  import org.apache.spark.sql.SparkSession
  val spark: SparkSession = SparkSession.builder
  .appName("My Spark Application")  // optional and will be autogenerated if not specified
  .master("local[*]")               // avoid hardcoding the deployment environment
  .enableHiveSupport()              // self-explanatory, isn't it?
  .config("spark.sql.warehouse.dir", "target/spark-warehouse")
  .getOrCreate
 
 *** NOTE: You can have multiple SparkSessions in a single Spark application.
 
 
 Implicit Conversions — implicits object
 - The implicits object is a helper class with the Scala implicit methods to convert Scala objects to Datasets, DataFrames and Columns. 
 - It also defines Encoders for Scala’s "primitive" types, e.g. Int, Double, String, and their products and collections.
 - implicits object offers support for CREATING Dataset from RDD of any type, case classes, tuples and Seq.
 - implicits object also offers conversions from Scala’s Symbol or $ to Column
 - It also offers conversions from RDD or Seq of Product types (e.g. case classes or tuples) to DataFrame.
 
      val spark = SparkSession.builder.getOrCreate()
      import spark.implicits._

NOTE: It is only possible to call toDF() on RDD objects of Int, Long, and String primitive types. 


 Encoders ( Ref link:https://mallikarjuna_g.gitbooks.io/spark/content/spark-sql-Encoder.html )
 - Encoder is the fundamental concept in the serialization and deserialization (SerDe) framework in Spark SQL 2.0. 
 - Encoders are defined in Spark SQL 2.0 as Encoder[T] trait.
 - An encoder of type T, i.e. Encoder[T], is used to convert (encode and decode) any JVM object or primitive of type T to and from Spark SQL’s InternalRow.
 - InternalRow is the internal binary row format representation.
 
 trait Encoder[T] extends Serializable {
  def schema: StructType
  def clsTag: ClassTag[T]
}
 
 
Deploy Mode
- Distinguishes where the Driver process runs. 
- It can be cluster mode or client mode
    - cluster mode - Spark launches the driver inside of the cluster. 
    - client mode - the submitter launches the driver outside of the cluster.

Resource Manager
- Component that controls, governs, and reserves computing resources in the form of containers on the cluster.
- These containers are reserved by request of Application Master and are allocated to Application Master when they are released or available.
- SparkContext can connect to different types of Cluster Managers. Now the most popular types are Spark's own standalone cluster manager, YARN, Mesos, Kubernetes etc.


- Executor
- A process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them. 
- Each application has its own executors.
- Executors keep on updating their status to Driver program.


Task 
- A unit of work that will be sent to one executor.
**** - A single task will operate on a single partition of RDD

Job (private[spark] class org.apache.spark.scheduler.ActiveJob)( Ref link : https://mallikarjuna_g.gitbooks.io/spark/content/spark-dagscheduler-jobs.html)
- A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. save, collect). Actions are EXPLAINED Above.
- RDD actions submit jobs to DAGScheduler
- A job can be one of two logical types;
    1. Map-stage job -  it computes the map output files for a ShuffleMapStage.
    2. Result job - it computes a ResultStage to execute an action.

NOTE:
- Not all partitions have always to be computed for ResultStages for actions like first() and lookup(). 

Stage
- Each job gets divided into smaller sets of tasks called stages that depend on each other (similar to the map and reduce stages in MapReduce)
- A stage is a physical unit of execution. It is a step in a physical execution plan.
- A stage is a set of parallel tasks(TaskSets), one per partition of an RDD, that compute partial results of a function executed as part of a Spark job.
**** Refer Diagram here - https://mallikarjuna_g.gitbooks.io/spark/content/spark-dagscheduler-stages.html
- In other words, a Spark job slices into stages.
- A stage is uniquely identified by id.
- When a stage is created, DAGScheduler increments internal counter nextStageId to track the number of stage submissions.
- A stage can only work on the partitions of a single RDD, but can be associated with many other dependent parent stages, with the boundary of a stage marked by shuffle dependencies.
- Submitting a stage can therefore trigger execution of a series of dependent parent stages.
- Every stage has a firstJobId that is the id of the job that submitted the stage.
- Stages are of 2 types:
    1. ShuffleMapStage
        - Intermediate stage (in the execution DAG) that produces data for other stage(s). 
        - It is an input for the other following stages in the DAG of stages. 
        - It writes map output files for a shuffle. It can also be the final stage in a job in Adaptive Query Planning / Adaptive Scheduling.
    2. ResultStage 
        - It is the final stage that executes a Spark action in a user program by running a function on an RDD.
        - ResultStage applies a function on one or many partitions of the target RDD to compute the result of an action.

TaskSet 
- collection of tasks that belong to a single stage and a stage attempt. 
- A TaskSet contains a fully-independent sequence of tasks that can run right away based on the data that is already on the cluster

RDD Shuffling
- Shuffling is a process of redistributing data across partitions (aka repartitioning) that may or may not cause moving data across JVM processes or even over the wire (between executors on separate machines).
- Shuffling is the process of data transfer between stages.

DAGScheduler
- DAGScheduler is the scheduling layer of Apache Spark that implements STAGE-ORIENTED scheduling.
- After an action has been called, SparkContext hands over a logical plan to DAGScheduler.
- It transforms a logical execution plan (i.e. RDD lineage of dependencies built using RDD transformations) to a physical execution plan using STAGES.
- DAGScheduler in turn translates to a set of stages that are submitted as TaskSets for execution.
- DAGScheduler does three things in Spark (thorough explanations follow):
    - Computes an execution DAG of stages, keeps track of which RDDs and stage outputs are materialized, and finds a minimal schedule to run jobs and at end submits stages to TaskScheduler. 
    - Determines the preferred locations to run each task on.
***   - Handles failures due to shuffle output files being lost.

NOTE:
*** - DAGScheduler handles failures due to shuffle output files being lost, in which case old stages may need to be resubmitted.
- Failures within a stage that are not caused by shuffle file loss are handled by the TaskScheduler itself, which retry each task a small number of times before cancelling the whole stage.

- TaskScheduler. ( org.apache.spark.scheduler - private[spark] trait TaskScheduler)
- A TaskScheduler gets sets of tasks (as TaskSets) submitted to it from the DAGScheduler for each stage, 
- It is responsible for sending the tasks to the cluster, running them, retrying if there are failures, and mitigating stragglers.
- A TaskScheduler is created while SparkContext is being created (by calling SparkContext.createTaskScheduler for a given master URL and deploy mode).

    def createTaskScheduler(
       sc: SparkContext,
       master: String,
       deployMode: String): (SchedulerBackend, TaskScheduler)  // RETURNS TUPLE

- Above method is called at time of creating an instance of SparkContext to create TaskScheduler and SchedulerBackend objects.
- createTaskScheduler understands the following master URLs:
      local - local mode with 1 thread only
      local[n] or local[*] - local mode with n threads.
      local[n, m] or local[*, m] — local mode with n threads and m number of failures.
      spark://hostname:port for Spark Standalone.
      local-cluster[n, m, z] — local cluster with n workers, m cores per worker, and z memory per worker.
      mesos://  - for Spark on Mesos cluster
      yarn - for Spark on YARN

SchedulerBackend
- SchedulerBackend is a pluggable backend mechanism (aka backend scheduler) to support various cluster managers like Apache Mesos, Hadoop YARN, Spark’s own Spark Standalone and Spark local.
- Above cluster managers differ by their custom task scheduling modes and resource offers mechanisms, and Spark’s approach is to abstract the differences in SchedulerBackend Contract.

BlockManager ( Ref https://programmer.help/blogs/disk-store-disk-store-for-spark-storage-management.html)
Ref link - https://mallikarjuna_g.gitbooks.io/spark/content/spark-blockmanager.html#stores
- Block Manager (BlockManager) is a key-value store for blocks of data in Spark.
- Blockmanager is a class that actually has an instance running (including driver and executor processes) on every node at runtime. 
- BlockManager acts as a LOCAL CACHE, that runs on every "node" in a Spark application and is created right when SparkEnv.
- BlockManager provides interface for uploading and fetching blocks both locally and remotely using various Stores - MemoryStore, DiskStore, and ExternalBlockStore- OFF_HEAP.
- Whether it's creating broadcast variables on the driver side, writing shuffle blocks in the shuffle process on the executor side, or the result of the task runtime is too large to be transmitted through BlockManager, or the RDD cache, it will work on every running node.
- Blockmanager is used to manage the internal reading and writing of local memory and disks


SQLContext
- Entry point for SparkSQL module which deals with structured data in SQL way.
- Allows us to connect to different Data Sources to write or read data from them.
- Once SQLContext is initialised, the user can then use it in order to perform various “sql-like” operations over Datasets and Dataframes.
- In order to create a SQLContext, you first need to instantiate a SparkContext as shown below:

import org.apache.spark.{SparkContext, SparkConf}
import org.apache.spark.sql.SQLContextval sparkConf = new SparkConf() \
    .setAppName("app") \
    .setMaster("yarn")
val sc = new SparkContext(sparkConf)
val sqlContext = new SQLContext(sc)


HiveContext
- HiveContext is a Superset of SQLContext.
- Working on Hive table in Spark with Spark version less than 2.x needs HiveContext.
- It gives access of Hive tables, metatastores etc to Spark.
- For Spark 1.5+, HiveContext also offers support for window functions.

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.hive.HiveContextval sparkConf = new SparkConf() \
    .setAppName("app") \
    .setMaster("yarn")
val sc = new SparkContext(sparkConf)
val hiveContext = new HiveContext(sc)hiveContext.sql("select * from tableName limit 0")


SparkSession
- Spark 2.0 introduced SparkSession and it is a combination of all these different contexts.
- Note that the old SQLContext and HiveContext are kept for backward compatibility.
- Additionally, it gives to developers immediate access to SparkContext.

import org.apache.spark.sql.SparkSessionval sparkSession = SparkSession \
    .builder() \
    .appName("myApp") \
    .enableHiveSupport() \
    .getOrCreate()// Two ways you can access spark context from spark session
val spark_context = sparkSession._sc                                                // Approach 1
val spark_context = sparkSession.sparkContext                                       // Approach 2



What is SparkEnv?
block eviction in spark?
Result job that computes a ResultStage to execute an action.? - https://medium.com/@gkadam2011/beneath-rdd-resilient-distributed-dataset-in-apache-spark-260c0b7250c6
Types of RDDs?
Catalyst Optimizer?
Tungsten Engine?


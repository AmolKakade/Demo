Driver, Executor, Various Contexts, architecture, Spark Master
RDD, DataSet
SparkSession
Stages, Job, Tasks
DAG, DAG Scheduler
Encoders
SparkConf
Block Manager
Product types?
what is the need of directed acyclic graph in Spark, how to create DAG in Spark and how it helps in achieving fault tolerance.
difference between Apache Spark and Hadoop MapReduce

ref links: 
https://mallikarjuna_g.gitbooks.io/spark/content/spark-dagscheduler.html
https://luminousmen.com/post/spark-anatomy-of-spark-application
https://spark.apache.org/docs/latest/cluster-overview.html


Driver
- It is the process running the main() function of the application and creating the SparkContext.
- The Driver is responsible for converting a user application to smaller execution units called TASKS and then SCHEDULES them to run with a cluster manager on executors.
- The Driver is also responsible for executing the Spark application and returning the status/results to the user.
- Spark Driver contains various components 
  – DAGScheduler
  - TaskScheduler
  - BackendScheduler
  - BlockManager

- Driver properties:
  - Runs as a seperate process in its own JVM.
  - stores metadata about all Resilient Distributed Databases and their partitions;
  - is created after the user sends the Spark application to the cluster manager (YARN in our case);
  - optimizes logical DAG transformations and, if possible, combines them in stages and determines the best location for execution of this DAG;
  - creates Spark WebUI with detailed information about the application;
  

SparkContext ( org.apache.spark -  class SparkContext extends Logging )
- Entry point for Spark application.
- SparkContext can connect to several types of cluster managers ( Spark standalone cluster manager, Mesos, YARN, Kubernetes), which allocate resources across applications.
- A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster. 
- Spark Context also tracks executors in real-time by sending regular heartbeat messages.
- Spark Context stops working after the Spark application is finished.

 SparkSession 
 - SparkSession is the entry point to Spark SQL. 
 - It is the very first object you have to create to start developing Spark SQL applications.
 **** - SparkSession has merged SQLContext and HiveContext in one object as of Spark 2.0.0. 
 
    - You use the SparkSession.builder method to create an instance of SparkSession.
    
  import org.apache.spark.sql.SparkSession
  val spark: SparkSession = SparkSession.builder
  .appName("My Spark Application")  // optional and will be autogenerated if not specified
  .master("local[*]")               // avoid hardcoding the deployment environment
  .enableHiveSupport()              // self-explanatory, isn't it?
  .config("spark.sql.warehouse.dir", "target/spark-warehouse")
  .getOrCreate
 
 *** NOTE: You can have multiple SparkSessions in a single Spark application.
 
 
 Implicit Conversions — implicits object
 - The implicits object is a helper class with the Scala implicit methods to convert Scala objects to Datasets, DataFrames and Columns. 
 - It also defines Encoders for Scala’s "primitive" types, e.g. Int, Double, String, and their products and collections.
 - implicits object offers support for CREATING Dataset from RDD of any type, case classes, tuples and Seq.
 - implicits object also offers conversions from Scala’s Symbol or $ to Column
 - It also offers conversions from RDD or Seq of Product types (e.g. case classes or tuples) to DataFrame.
 
      val spark = SparkSession.builder.getOrCreate()
      import spark.implicits._

NOTE: It is only possible to call toDF() on RDD objects of Int, Long, and String primitive types. 


 Encoders ( Ref link:https://mallikarjuna_g.gitbooks.io/spark/content/spark-sql-Encoder.html )
 - Encoder is the fundamental concept in the serialization and deserialization (SerDe) framework in Spark SQL 2.0. 
 - Encoders are defined in Spark SQL 2.0 as Encoder[T] trait.
 - An encoder of type T, i.e. Encoder[T], is used to convert (encode and decode) any JVM object or primitive of type T to and from Spark SQL’s InternalRow.
 - InternalRow is the internal binary row format representation.
 
 trait Encoder[T] extends Serializable {
  def schema: StructType
  def clsTag: ClassTag[T]
}
 
 
Deploy Mode
- Distinguishes where the Driver process runs. 
- It can be cluster mode or client mode
    - cluster mode - Spark launches the driver inside of the cluster. 
    - client mode - the submitter launches the driver outside of the cluster.

Resource Manager
- Component that controls, governs, and reserves computing resources in the form of containers on the cluster.
- These containers are reserved by request of Application Master and are allocated to Application Master when they are released or available.
- SparkContext can connect to different types of Cluster Managers. Now the most popular types are Spark's own standalone cluster manager, YARN, Mesos, Kubernetes etc.


- Executor
- A process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them. 
- Each application has its own executors.
- Executors keep on updating their status to Driver program.


Task 
- A unit of work that will be sent to one executor.
**** - A single task will operate on a single partition of RDD

Job
- A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. save, collect)

Stage
- Each job gets divided into smaller sets of tasks called stages that depend on each other (similar to the map and reduce stages in MapReduce)
- A stage is a physical unit of execution. It is a step in a physical execution plan.
- A stage is a set of parallel tasks(TaskSets), one per partition of an RDD, that compute partial results of a function executed as part of a Spark job.
**** Refer Diagram here - https://mallikarjuna_g.gitbooks.io/spark/content/spark-dagscheduler-stages.html
- In other words, a Spark job slices into stages.
- A stage is uniquely identified by id.
- When a stage is created, DAGScheduler increments internal counter nextStageId to track the number of stage submissions.
- A stage can only work on the partitions of a single RDD, but can be associated with many other dependent parent stages, with the boundary of a stage marked by shuffle dependencies.
- Submitting a stage can therefore trigger execution of a series of dependent parent stages.
- Every stage has a firstJobId that is the id of the job that submitted the stage.
- Stages are of 2 types:
    1. ShuffleMapStage
        - Intermediate stage (in the execution DAG) that produces data for other stage(s). 
        - It is an input for the other following stages in the DAG of stages. 
        - It writes map output files for a shuffle. It can also be the final stage in a job in Adaptive Query Planning / Adaptive Scheduling.
    2. ResultStage 
        - It is the final stage that executes a Spark action in a user program by running a function on an RDD.

TaskSet 
- collection of tasks that belong to a single stage and a stage attempt. 
- A TaskSet contains a fully-independent sequence of tasks that can run right away based on the data that is already on the cluster

RDD Shuffling
- Shuffling is a process of redistributing data across partitions (aka repartitioning) that may or may not cause moving data across JVM processes or even over the wire (between executors on separate machines).
- Shuffling is the process of data transfer between stages.

DAGScheduler
- DAGScheduler is the scheduling layer of Apache Spark that implements STAGE-ORIENTED scheduling.
- After an action has been called, SparkContext hands over a logical plan to DAGScheduler.
- It transforms a logical execution plan (i.e. RDD lineage of dependencies built using RDD transformations) to a physical execution plan using STAGES.
- DAGScheduler in turn translates to a set of stages that are submitted as TaskSets for execution.
- DAGScheduler does three things in Spark (thorough explanations follow):
    - Computes an execution DAG of stages, keeps track of which RDDs and stage outputs are materialized, and finds a minimal schedule to run jobs and at end submits stages to TaskScheduler. 
    - Determines the preferred locations to run each task on.
***   - Handles failures due to shuffle output files being lost.

NOTE:
*** - DAGScheduler handles failures due to shuffle output files being lost, in which case old stages may need to be resubmitted.
- Failures within a stage that are not caused by shuffle file loss are handled by the TaskScheduler itself, which retry each task a small number of times before cancelling the whole stage.

- TaskScheduler. ( org.apache.spark.scheduler - private[spark] trait TaskScheduler)
- A TaskScheduler gets sets of tasks (as TaskSets) submitted to it from the DAGScheduler for each stage, 
- It is responsible for sending the tasks to the cluster, running them, retrying if there are failures, and mitigating stragglers.
- A TaskScheduler is created while SparkContext is being created (by calling SparkContext.createTaskScheduler for a given master URL and deploy mode).

    def createTaskScheduler(
       sc: SparkContext,
       master: String,
       deployMode: String): (SchedulerBackend, TaskScheduler)  // RETURNS TUPLE

- Above method is called at time of creating an instance of SparkContext to create TaskScheduler and SchedulerBackend objects.
- createTaskScheduler understands the following master URLs:
      local - local mode with 1 thread only
      local[n] or local[*] - local mode with n threads.
      local[n, m] or local[*, m] — local mode with n threads and m number of failures.
      spark://hostname:port for Spark Standalone.
      local-cluster[n, m, z] — local cluster with n workers, m cores per worker, and z memory per worker.
      mesos://  - for Spark on Mesos cluster
      yarn - for Spark on YARN

SchedulerBackend
- SchedulerBackend is a pluggable backend mechanism (aka backend scheduler) to support various cluster managers like Apache Mesos, Hadoop YARN, Spark’s own Spark Standalone and Spark local.
- Above cluster managers differ by their custom task scheduling modes and resource offers mechanisms, and Spark’s approach is to abstract the differences in SchedulerBackend Contract.

BlockManager ( Ref https://programmer.help/blogs/disk-store-disk-store-for-spark-storage-management.html)
Ref link - https://mallikarjuna_g.gitbooks.io/spark/content/spark-blockmanager.html#stores
- Block Manager (BlockManager) is a key-value store for blocks of data in Spark.
- Blockmanager is a class that actually has an instance running (including driver and executor processes) on every node at runtime. 
- BlockManager acts as a LOCAL CACHE, that runs on every "node" in a Spark application and is created right when SparkEnv.
- BlockManager provides interface for uploading and fetching blocks both locally and remotely using various Stores - MemoryStore, DiskStore, and ExternalBlockStore- OFF_HEAP.
- Whether it's creating broadcast variables on the driver side, writing shuffle blocks in the shuffle process on the executor side, or the result of the task runtime is too large to be transmitted through BlockManager, or the RDD cache, it will work on every running node.
- Blockmanager is used to manage the internal reading and writing of local memory and disks





What is SparkEnv?
block eviction in spark?

1. Mapper, Combiner (if any), Partitioner, Reducer, InputFormat and OutputFormat, JobConf
2. Writables
3. Context


1. Mapper. ( Class Mapper<KEYIN,VALUEIN,KEYOUT,VALUEOUT>)
- Mapper maps input key/value pairs(KEYIN,VALUEIN) to a set of intermediate key/value pairs(KEYOUT,VALUEOUT).
- Mapper implementations are passed to the job via Job.setMapperClass(Class) method. 
- The framework then calls map(WritableComparable, Writable, Context) for each key/value pair in the InputSplit for that task.

*****NOTE:
- Number of Map tasks = Number of InputSplits
- Number of Times map() method will be called = Number of key/value pairs(KEYIN,VALUEIN) in InputSplit

- Map Task:
      - Maps are the individual tasks that transform input records into intermediate records. 
      - The transformed intermediate records do not need to be of the same type as the input records.
      - A given input pair may map to zero or many output pairs.
      - The Hadoop MapReduce framework spawns one map task for each InputSplit generated by the InputFormat for the job.

2. InputSplit ( org.apache.hadoop.mapreduce  - Class InputSplit)
- InputSplit represents the data to be processed by an individual Mapper.
- InputSplit presents a byte-oriented view on the input and is the responsibility of RecordReader of the job to process this and present a record-oriented view.
- Below property used ti change size of InputSplit block.

*** NOTE: mapreduce.input.fileinputformat.split.maxsize is used to control, size of split

3. RecordReader ( org.apache.hadoop.mapreduce - Class RecordReader<KEYIN,VALUEIN>)
- The record reader breaks the data into key/value pairs for input to the Mapper class based on InputFormat.

4. InputFormat ( org.apache.hadoop.mapreduce Class InputFormat<K,V>)
- InputFormat describes the input-specification for a Map-Reduce job.
- The Map-Reduce framework relies on the InputFormat of the job to:
      Validate the input-specification of the job.
      Split-up the input file(s) into logical InputSplits, each of which is then assigned to an individual Mapper.
      Provide the RecordReader implementation to be used to clean input records from the logical InputSplit for processing by the Mapper.
- Known sub classes are
      - FileInputFormat  
      - DBInputFormat  - A InputFormat that reads input data from an SQL table. 
      - CompositeInputFormat 
      - ComposableInputFormat

5. FileInputFormat ( org.apache.hadoop.mapreduce.lib.input - Class FileInputFormat<K,V>)
- It is a base class for file-based InputFormats. 
- This provides a generic implementation of getSplits(JobContext) inherited from InputFormat. 
- Implementations of FileInputFormat can also override the isSplitable(JobContext, Path) method to prevent input files from being split-up in certain situations. 
- Implementations that may deal with non-splittable files must override this method,

6. DistributedCache ( org.apache.hadoop.mapreduce.filecache.DistributedCache)

- Distribute application-specific large, read-only files efficiently. 
- DistributedCache is a facility provided by the Map-Reduce framework to cache files (text, archives, jars etc.) needed by applications.
- Applications specify the files, via urls (hdfs:// or http://) to be cached via the JobConf. 
- The DistributedCache assumes that the files specified via urls are already present on the FileSystem at the path specified by the url and are accessible by every machine in the cluster.
- The framework will copy the necessary files on to the slave node before any tasks for the job are executed on that node.

e.g.

// Setting up the cache for the application
     
-Copy the requisite files to the FileSystem:
     $ bin/hadoop fs -copyFromLocal lookup.dat /myapp/lookup.dat        ( ***** file)
     $ bin/hadoop fs -copyFromLocal map.zip /myapp/map.zip              ( ***** zip/archive)
     $ bin/hadoop fs -copyFromLocal mylib.jar /myapp/mylib.jar          ( ***** jar)
     $ bin/hadoop fs -copyFromLocal mytar.tar /myapp/mytar.tar
     $ bin/hadoop fs -copyFromLocal mytgz.tgz /myapp/mytgz.tgz  
     
-Setup the application's JobConf:    
    JobConf job = new JobConf();
     DistributedCache.addCacheFile(new URI("/myapp/lookup.dat#lookup.dat"), job);
     DistributedCache.addCacheArchive(new URI("/myapp/map.zip", job);
     DistributedCache.addFileToClassPath(new Path("/myapp/mylib.jar"), job);
     DistributedCache.addCacheArchive(new URI("/myapp/mytar.tar", job);
     DistributedCache.addCacheArchive(new URI("/myapp/mytgz.tgz", job);
     
-Use the cached files in the Mapper OR Reducer:
    // Get the cached archives/files
         File f = new File("./map.zip/some/file/in/zip.txt");

7. Reducer ( org.apache.hadoop.mapreduce. - Class Reducer<KEYIN,VALUEIN,KEYOUT,VALUEOUT>)
- Reduces a set of intermediate values FOR A SAME key to a smaller set of values. 
- Reducer implementations can access the Configuration for the job via the JobContext.getConfiguration() method.
- Reducer has 3 primary phases:
        1. Shuffle 
            - The Reducer copies the sorted output from each Mapper using HTTP across the network.
        2. Sort
            - The framework merge sorts Reducer inputs by keys (since different Mappers may have output the same key).
***** NOTE: The shuffle and sort phases occur simultaneously. Idea if shuffle and sort is to make Reducer available all values for a key.
        3. Reduce
            - In this phase the reduce(Object, Iterable, org.apache.hadoop.mapreduce.Reducer.Context) method is called for each <key, (collection of values)> in the sorted inputs.
            - The output of the reduce task is typically written to a RecordWriter via TaskInputOutputContext.write(Object, Object).

 public class IntSumReducer<Key> extends Reducer<Key,IntWritable,Key,IntWritable> {
   private IntWritable result = new IntWritable();
   public void reduce(Key key, Iterable<IntWritable> values,Context context) throws IOException, InterruptedException {
     int sum = 0;
     for (IntWritable val : values) {
       sum += val.get();
     }
     result.set(sum);
     context.write(key, result);
   }
 }

8. Shuffle and Sort
Shuffle:
- Shuffle and Sort ARE PART OF REDUCER ONLY. ********(shown above)
- The process of transferring data from the mappers to reducers is known as shuffling, thus its MANDOTAORY for MapReduce
- This is the process by which the system performs the sort and transfers the map output to the reducer as input. 
- Shuffling can start even before the map phase has finished so this saves some time and completes the tasks in lesser time.

Sort:
- The keys generated by the mapper are automatically sorted by MapReduce Framework.
- Before starting of actual reducer operation, all intermediate key-value pairs in MapReduce that are generated by mapper get sorted by key and not by value. 
- Values passed to each reducer are not sorted; they can be in any order.
- Sorting in Hadoop helps reducer to easily distinguish when a new reduce task should start. 


9. Partioner ( org.apache.hadoop.mapreduce  - Class Partitioner<KEY,VALUE>)
- The partition phase takes place after the Map phase and before the Reduce phase.
- Multiple keys are combined together with a hash function to generate partition.
- The number of partitions is equal to the number of reducers. That means a partitioner will divide the data according to the number of reducers. 
- Therefore, the data passed from a single partitioner is processed by a single Reducer.

NOTE: 
- A Partitioner is created only when there are multiple reducers.
- Number of partitions = JobConf.setNumReduceTasks() 

10. Custom Partitioner
- By default the partitioner implementation is called HashPartitioner. 
- It uses the hashCode() method of the KEY objects modulo the number of partitions total to determine which partition to send a given (key, value) pair to.
- Partitioner provides the getPartition() method that you can implement yourself if you want to declare the custom partition for your job.
- The getPartition() method takes a <K,V> pair and the number of partitions to split the data, a number in the range [0, numPartitions) must be returned by this method.

public static class MyPartitioner extends Partitioner<Text,Text>{
public int getPartition(Text key, Text value, int numReduceTasks){
if(numReduceTasks==0)
return 0;
if(key.equals(new Text(“Male”)) )
return 0;
if(key.equals(new Text(“Female”)))
return 1;
}
}

NOTE:
- Here , the getPartition() will return 0 if the key is Male and 1 if key is Female.
- We can check our output in two files:
part-r-0000 and part-r-0001.

11. Secondary Sort ( Ref link - https://www.oreilly.com/library/view/data-algorithms/9781491906170/ch01.html)
- A secondary sort problem relates to sorting VALUES associated with a KEY in the reduce phase.
- The secondary sorting technique will enable us to sort the values (in ascending or descending order) passed to each reducer.

12. Counter ( org.apache.hadoop.mapreduce. - Interface Counter)
- A named counter that tracks the progress of a map/reduce job.
- Counters represent global counters, defined either by the Map-Reduce framework or applications. 
- Each Counter is named by an Enum and has a long for the value.
- Counters are bunched into Groups, each comprising of counters from a particular Enum class.
- We can even implement our own CUSTOM COUNTERS.

E.g.
    The correct number of bytes was read and written.
    The correct number of tasks was launched and successfully ran.
    The amount of CPU and memory consumed is appropriate for our job and cluster nodes.
    
   

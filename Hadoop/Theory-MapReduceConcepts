1. Writable ( org.apache.hadoop.io - Interface Writable)
---------------------------------------------------------------------------------------------------------------------------------------------------
- A serializable object which implements a simple, efficient, serialization protocol, based on DataInput and DataOutput.
- Any key or value type in the Hadoop Map-Reduce framework implements this interface.
- Known Implementing classes are - ByteWritable, IntWritable, LongWritable, ShortWritable, DoubleWritable, FloatWritable, Text, ArrayWritable etc.

DataInput ( public interface DataInput) 
- This is interface provided by Java. 
- The DataInput interface provides for reading bytes from a binary stream and reconstructing from them data in any of the Java primitive types. 
- It is generally true of all the reading routines in this interface that if end of file is reached before the desired number of bytes has been read, an EOFException (which is a kind of IOException).
- If any byte cannot be read for any reason other than end of file, an IOException other than EOFException is thrown. 
- In particular, an IOException may be thrown if the input stream has been closed. 

DataOutput ( public interface DataOutput) 
- The DataOutput interface provides for converting data from any of the Java primitive types to a series of bytes and writing these bytes to a binary stream. 
- For all the methods in this interface that write bytes, it is generally true that if a byte cannot be written for any reason, an IOException is thrown.


2. Mapper. ( org.apache.hadoop.mapreduce.Mapper - Class Mapper<KEYIN,VALUEIN,KEYOUT,VALUEOUT>)
---------------------------------------------------------------------------------------------------------------------------------------------------
- Mapper maps input key/value pairs(KEYIN,VALUEIN) to a set of intermediate key/value pairs(KEYOUT,VALUEOUT).
- Mapper implementations are passed to the job via Job.setMapperClass(Class) method. 
- The framework then calls map() for each key/value pair in the InputSplit for that task.

E.g. - 1
public void map(Object key, Text value, Context context
                    ) throws IOException, InterruptedException

*****NOTE:
- Number of Map tasks = Number of InputSplits
- Number of Times map() method will be called = Number of key/value pairs(KEYIN,VALUEIN) in InputSplit

******How Many Maps?
- The number of maps is usually driven by the total size of the inputs, that is, the total number of blocks of the input files.
- The right level of parallelism for maps seems to be around 10-100 maps per-node, although it has been set up to 300 maps for very cpu-light map tasks. 
- TaskSetup() from OutputCommitter class takes a while, so it is best if the maps take at least a minute to execute.
- Thus, if you expect 10TB of input data and have a blocksize of 128MB, you’ll end up with 82,000 maps ( 8 * 1024 * 10)
*****- Configuration.set(MRJobConfig.NUM_MAPS, int) (which only provides a hint to the framework) is used to set it even higher.

- Map Task:
      - Maps are the individual tasks that transform input records into intermediate records. 
      - The transformed intermediate records do not need to be of the same type as the input records.
      - A given input pair may map to zero or many output pairs.
      - The Hadoop MapReduce framework spawns one map task for each InputSplit generated by the InputFormat for the job.


3. IdentityMapper ( org.apache.hadoop.mapred.lib - Class IdentityMapper<K,V>)
---------------------------------------------------------------------------------------------------------------------------------------------------
- Implements the identity( as is) function, mapping inputs directly to outputs.
- It is a generic class and also the default mapper class provided by the Hadoop.
- When no mapper class is specified in the MR Driver class, the Identity Mapper class is invoked automatically when a Map-Reduce job is assigned.
- This will read all input and write and key value pair as is.


4. ChainMapper ( org.apache.hadoop.mapreduce.lib.chain - Class ChainMapper<KEYIN,VALUEIN,KEYOUT,VALUEOUT>)
---------------------------------------------------------------------------------------------------------------------------------------------------
- The ChainMapper class allows to use multiple Mapper classes within a single Map task.
- The Mapper classes are invoked in a chained (or piped) fashion, the output of the first becomes the input of the second, and so on.
- The last Mapper writes to the task's output. 
- The key functionality of this feature is that the Mappers in the chain do not need to be aware that they are executed in a chain. 

NOTE:
- There is no need to specify the output key/value classes for the ChainMapper, this is done by the addMapper for the last mapper in the chain. 

E.g.
 job = new Job(conf);
 Configuration mapAConf = new Configuration(false);
 ...
 ChainMapper.addMapper(job, AMap.class, LongWritable.class, Text.class, Text.class, Text.class, true, mapAConf);

 Configuration mapBConf = new Configuration(false);
 ...
 ChainMapper.addMapper(job, BMap.class, Text.class, Text.class, LongWritable.class, Text.class, false, mapBConf);
 

5. OutputCommitter  ( org.apache.hadoop.mapreduce - Class OutputCommitter)
---------------------------------------------------------------------------------------------------------------------------------------------------
- OutputCommitter describes the commit of task output for a Map-Reduce job.
- The Map-Reduce framework relies on the OutputCommitter of the job to:
      - Setup the job during initialization. For example, create the temporary output directory for the job during the initialization of the job.
            setupJob(JobContext jobContext)
            
      - Cleanup the job after the job completion. For example, remove the temporary output directory after the job completion.
             commitJob(JobContext) and abortJob(JobContext, JobStatus.State) 
             
      - Setup the task temporary output.
            setupTask(TaskAttemptContext taskContext)
            
      - Check whether a task needs a commit. This is to avoid the commit procedure if a task does not need commit.
      - Commit of the task output.
            commitTask(TaskAttemptContext taskContext)

      - Discard the task commit.


6. JobStatus ( org.apache.hadoop.mapred - Class JobStatus)
---------------------------------------------------------------------------------------------------------------------------------------------------
- During its lifetime MapReduce job will go in vaious phases of execution.
- Describes the current status of a job. This is not intended to be a comprehensive piece of data. For that, look at JobProfile.

      public static final int PREP
      public static final int RUNNING
      public static final int SUCCEEDED
      public static final int FAILED
      public static final int KILLED


7. InputSplit ( org.apache.hadoop.mapreduce  - Class InputSplit)
---------------------------------------------------------------------------------------------------------------------------------------------------
- InputSplit represents the data to be processed by an individual Mapper.
- InputSplit presents a byte-oriented view on the input and is the responsibility of RecordReader of the job to process this and present a record-oriented view.
- Below property used to change size of InputSplit block.

*** NOTE: mapreduce.input.fileinputformat.split.maxsize is used to control, size of split


8. RecordReader ( org.apache.hadoop.mapreduce - Class RecordReader<KEYIN,VALUEIN>)
---------------------------------------------------------------------------------------------------------------------------------------------------
- The record reader breaks the data into key/value pairs for input to the Mapper class based on InputFormat.


9. InputFormat ( org.apache.hadoop.mapreduce Class InputFormat<K,V>)
---------------------------------------------------------------------------------------------------------------------------------------------------
- InputFormat describes the input-specification for a Map-Reduce job.
- The Map-Reduce framework relies on the InputFormat of the job to:
      Validate the input-specification of the job.
      Split-up the input file(s) into logical InputSplits, each of which is then assigned to an individual Mapper.
      Provide the RecordReader implementation to be used to clean input records from the logical InputSplit for processing by the Mapper.
- Known sub classes are
      - FileInputFormat  
      - DBInputFormat  - A InputFormat that reads input data from an SQL table. 
      - CompositeInputFormat 
      - ComposableInputFormat


10. FileInputFormat ( org.apache.hadoop.mapreduce.lib.input - Class FileInputFormat<K,V>)
---------------------------------------------------------------------------------------------------------------------------------------------------
- It is a base class for file-based InputFormats. 
- This provides a generic implementation of getSplits(JobContext) inherited from InputFormat. 
- Implementations of FileInputFormat can also override the isSplitable(JobContext, Path) method to prevent input files from being split-up in certain situations. 
- Implementations that may deal with non-splittable files must override this method,


11. DistributedCache ( org.apache.hadoop.mapreduce.filecache.DistributedCache)
---------------------------------------------------------------------------------------------------------------------------------------------------
- Distribute application-specific large, read-only files efficiently. 
- DistributedCache is a facility provided by the Map-Reduce framework to cache files (text, archives, jars etc.) needed by applications.
- Applications specify the files, via urls (hdfs:// or http://) to be cached via the JobConf. 
- The DistributedCache assumes that the files specified via urls are already present on the FileSystem at the path specified by the url and are accessible by every machine in the cluster.
- The framework will copy the necessary files on to the slave node before any tasks for the job are executed on that node.

e.g.

// Setting up the cache for the application
     
-Copy the requisite files to the FileSystem:
     $ bin/hadoop fs -copyFromLocal lookup.dat /myapp/lookup.dat        ( ***** file)
     $ bin/hadoop fs -copyFromLocal map.zip /myapp/map.zip              ( ***** zip/archive)
     $ bin/hadoop fs -copyFromLocal mylib.jar /myapp/mylib.jar          ( ***** jar)
     $ bin/hadoop fs -copyFromLocal mytar.tar /myapp/mytar.tar
     $ bin/hadoop fs -copyFromLocal mytgz.tgz /myapp/mytgz.tgz  
     
-Setup the application's JobConf:    
    JobConf job = new JobConf();
     DistributedCache.addCacheFile(new URI("/myapp/lookup.dat#lookup.dat"), job);
     DistributedCache.addCacheArchive(new URI("/myapp/map.zip", job);
     DistributedCache.addFileToClassPath(new Path("/myapp/mylib.jar"), job);
     DistributedCache.addCacheArchive(new URI("/myapp/mytar.tar", job);
     DistributedCache.addCacheArchive(new URI("/myapp/mytgz.tgz", job);
     
-Use the cached files in the Mapper OR Reducer:
    // Get the cached archives/files
         File f = new File("./map.zip/some/file/in/zip.txt");


12. Reducer ( org.apache.hadoop.mapreduce. - Class Reducer<KEYIN,VALUEIN,KEYOUT,VALUEOUT>)
---------------------------------------------------------------------------------------------------------------------------------------------------
- Reduces a set of intermediate values FOR A SAME key to a smaller set of values. 
- Reducer implementations can access the Configuration for the job via the JobContext.getConfiguration() method.
- Reducer has 3 primary phases:
        1. Shuffle 
            - The Reducer copies the sorted output from each Mapper using HTTP across the network.
        2. Sort
            - The framework merge sorts Reducer inputs by keys (since different Mappers may have output the same key).
***** NOTE: The shuffle and sort phases occur simultaneously. Idea if shuffle and sort is to make Reducer available all values for a key.
        3. Reduce
            - In this phase the reduce(Object, Iterable, org.apache.hadoop.mapreduce.Reducer.Context) method is called for each <key, (collection of values)> in the sorted inputs.
            - The output of the reduce task is typically written to a RecordWriter via TaskInputOutputContext.write(Object, Object).

 public class IntSumReducer<Key> extends Reducer<Key,IntWritable,Key,IntWritable> {
   private IntWritable result = new IntWritable();
   public void reduce(Key key, Iterable<IntWritable> values,Context context) throws IOException, InterruptedException {
     int sum = 0;
     for (IntWritable val : values) {
       sum += val.get();
     }
     result.set(sum);
     context.write(key, result);
   }
 }


13. Shuffle and Sort
---------------------------------------------------------------------------------------------------------------------------------------------------
Shuffle:
- Shuffle and Sort ARE PART OF REDUCER ONLY. ********(shown above)
- The process of transferring data from the mappers to reducers is known as shuffling, thus its MANDOTAORY for MapReduce
- This is the process by which the system performs the sort and transfers the map output to the reducer as input. 
- Shuffling can start even before the map phase has finished so this saves some time and completes the tasks in lesser time.

Sort:
- The keys generated by the mapper are automatically sorted by MapReduce Framework.
- Before starting of actual reducer operation, all intermediate key-value pairs in MapReduce that are generated by mapper get sorted by key and not by value. 
- Values passed to each reducer are not sorted; they can be in any order.
- Sorting in Hadoop helps reducer to easily distinguish when a new reduce task should start. 


14. Partioner ( org.apache.hadoop.mapreduce  - Class Partitioner<KEY,VALUE>)
---------------------------------------------------------------------------------------------------------------------------------------------------
- The partition phase takes place after the Map phase and before the Reduce phase.
- Multiple keys are combined together with a hash function to generate partition.
- The number of partitions is equal to the number of reducers. That means a partitioner will divide the data according to the number of reducers. 
- Therefore, the data passed from a single partitioner is processed by a single Reducer.

NOTE: 
- A Partitioner is created only when there are multiple reducers.
- Number of partitions = JobConf.setNumReduceTasks() 


15. Custom Partitioner
---------------------------------------------------------------------------------------------------------------------------------------------------
- By default the partitioner implementation is called HashPartitioner. 
- It uses the hashCode() method of the KEY objects modulo the number of partitions total to determine which partition to send a given (key, value) pair to.
- Partitioner provides the getPartition() method that you can implement yourself if you want to declare the custom partition for your job.
- The getPartition() method takes a <K,V> pair and the number of partitions to split the data, a number in the range [0, numPartitions) must be returned by this method.

public static class MyPartitioner extends Partitioner<Text,Text>{
public int getPartition(Text key, Text value, int numReduceTasks){
if(numReduceTasks==0)
return 0;
if(key.equals(new Text(“Male”)) )
return 0;
if(key.equals(new Text(“Female”)))
return 1;
}
}

NOTE:
- Here , the getPartition() will return 0 if the key is Male and 1 if key is Female.
- We can check our output in two files:
part-r-0000 and part-r-0001.


16. Secondary Sort ( Ref link - https://www.oreilly.com/library/view/data-algorithms/9781491906170/ch01.html)
---------------------------------------------------------------------------------------------------------------------------------------------------
- A secondary sort problem relates to sorting VALUES associated with a KEY in the reduce phase.
- The secondary sorting technique will enable us to sort the values (in ascending or descending order) passed to each reducer.


17. Counter ( org.apache.hadoop.mapreduce. - Interface Counter)
---------------------------------------------------------------------------------------------------------------------------------------------------
- A named counter that tracks the progress of a map/reduce job.
- Counters represent global counters, defined either by the Map-Reduce framework or applications. 
- Each Counter is named by an Enum and has a long for the value.
- Counters are bunched into Groups, each comprising of counters from a particular Enum class.
- We can even implement our own CUSTOM COUNTERS.

E.g.
    The correct number of bytes was read and written.
    The correct number of tasks was launched and successfully ran.
    The amount of CPU and memory consumed is appropriate for our job and cluster nodes.
    
    
18. Configuration - ( org.apache.hadoop.conf  - Class Configuration)
---------------------------------------------------------------------------------------------------------------------------------------------------
- Provides access to configuration parameters. 
- Configurations are specified by resources. A resource contains a set of name/value pairs as XML data. 
- Resource can be
      - String - If resource named by a String, then the classpath is examined for a file with that name. 
      - Path - If resourcenamed by a Path, then the local filesystem is examined directly, without referring to the classpath.
- Unless explicitly turned off, Hadoop by default specifies two resources, loaded in-order from the classpath:

    core-default.xml: Read-only defaults for hadoop.
    core-site.xml: Site-specific configuration for a given hadoop installation.

Applications may add additional resources, which are loaded subsequent to these resources in the order they are added.


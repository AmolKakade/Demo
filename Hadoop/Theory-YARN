***NOTE:
MapReduce in hadoop-2.x maintains API compatibility with previous stable release (hadoop-1.x). This means that all MapReduce jobs should still run unchanged on top of YARN with just a recompile.

Need of YARN ( ref link : https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html)
--------------------------------------------------------------------------------------------------------------------------------------------------------------- 
- The fundamental idea of YARN is to split up the functionalities of resource management and job scheduling/monitoring into separate daemons.
- resource management
    - ResourceManager - The idea is to have a global ResourceManager (RM)
    - NodeManager
    
ResourceManager
---------------------------------------------------------------------------------------------------------------------------------------------------------------
- The ResourceManager and the NodeManager form the data-computation framework. 
- The ResourceManager is the ultimate authority that arbitrates resources among all the applications in the system. 
- The ResourceManager has two main components;
    1. Scheduler
       - responsible for allocating resources to the various running applications subject to familiar constraints of capacities, queues etc. 
       - The Scheduler is pure scheduler in the sense that it performs NO monitoring NO tracking of status for the application.
       - offers no guarantees about restarting failed tasks either due to application failure or hardware failures. 
       - The Scheduler performs its scheduling function based on the resource requirements of the applications
       - It does so based on the abstract notion of a resource Container which incorporates elements such as memory, cpu, disk, network etc.
       - The Scheduler has a pluggable policy which is responsible for partitioning the cluster resources among the various queues, applications etc. 
       - The current schedulers such as the CapacityScheduler and the FairScheduler would be some examples of plug-ins.

    2. ApplicationsManager
       - responsible for accepting job-submissions, negotiating JUST THE FIRST container for executing the ApplicationMaster 
       - also provides the service for restarting the ApplicationMaster container on failure.

NodeManager ( Ref link https://blog.cloudera.com/apache-hadoop-yarn-nodemanager/)
---------------------------------------------------------------------------------------------------------------------------------------------------------------
- The NodeManager is the per-machine framework agent who is responsible for containers, monitoring their resource usage (cpu, memory, disk, network) 
- It is also reponsible for reporting the same to the ResourceManager/Scheduler.

Sub-Components of NodeManager:
1. NodeStatusUpdater
    - On cluster startup, NodeStatusUpdater registers with the RM and sends information about the resources available on the node. 
    - It keeps RM updated with information like - new containers running on the node, completed containers, etc.
    - In addition the RM may signal the NodeStatusUpdater to potentially kill already running containers.
2. ContainerManager
    - This is the core of the NodeManager and has below sub components;
        RPC server 
            - ContainerManager accepts requests from Application Masters (AMs) to start new containers, or to stop running ones.
            
        ResourceLocalizationService
            - Responsible for securely DOWNLOADING and organizing various file resources needed by containers.
            
        ContainersLauncher
            - Maintains a pool of threads to prepare and launch containers as quickly as possible. 
            - Also cleanup the containers processes when such a request is sent by the RM or the ApplicationMasters (AMs).
            
        ContainersMonitor
            - After a container is launched, this component starts observing its resource utilization while the container is running. 
            - To enforce isolation and fair sharing of resources like memory, each container is allocated some amount of such a resource by the RM. 
            - The ContainersMonitor monitors each container’s usage continuously and if a container exceeds its allocation, it signals the container to be killed. 
        
        LogHandler
            - A pluggable component with the option of either keeping the containers’ logs on the local disks or zipping them together and uploading them onto a file-system.

3. ContainerExecutor
    - Interacts with the underlying OS to place files and directories needed by containers and subsequently to launch/clean-up processes corresponding to containers.

4. WebServer
    - Exposes the information about list of applications, containers running, node-health related information and the logs produced by the containers.


ApplicationMaster
---------------------------------------------------------------------------------------------------------------------------------------------------------------
- There will be one ApplicationMaster (AM) per-application.
- It has the responsibility of negotiating appropriate resource containers from the Scheduler, tracking their status and monitoring for progress.
- An application is either a single job or a DAG of jobs.
- ApplicationMaster is a framework specific library does;
    - negotiating resources from the ResourceManager(Scheduler) 
    - working with the NodeManager(s) to execute and monitor the tasks.


FIFO scheduler
---------------------------------------------------------------------------------------------------------------------------------------------------------------
- This places applications in a queue and runs them in the order of submission (first in, first out). 
- It is not desirable, as a long-running application might block the small running applications.


CapacityScheduler
---------------------------------------------------------------------------------------------------------------------------------------------------------------
- The CapacityScheduler is designed to allow sharing a large cluster used by multiple organizations, while giving each organization capacity guarantees. 
- The central idea is that the available resources in the Hadoop cluster are shared among multiple organizations who collectively fund the cluster based on their computing needs. 
- There is an added benefit that an organization can access any excess capacity not being used by others. 
- This provides elasticity for the organizations in a cost-effective manner.
- CapacityScheduler provides limits on initialized & pending applications from a single user and queue to ensure fairness and stability of the cluster.
- The primary abstraction provided by the CapacityScheduler is the concept of QUEUES. These queues are typically setup by administrators.
- CapacityScheduler supports hierarchical queues to ensure resources are shared among the sub-queues of an organization before other queues are allowed to use free resources
- Features of CapacityScheduler;
    Hierarchical Queues - Hierarchy of queues is supported to ensure resources are shared among the sub-queues of an organization before with other queues.
    Capacity Guarantees - Queues are allocated a fraction of the capacity of the grid in the sense that a certain capacity of resources will be at their disposal. All applications submitted to a queue will have access to the capacity allocated to the queue. Administrators can configure soft limits and optional hard limits on the capacity allocated to each queue.
    Security - Each queue has strict ACLs which controls which users can submit applications to individual queues. Also, per-queue and system administrator roles are supported.
    Elasticity - Free resources can be allocated to any queue beyond its capacity.
    Multi-tenancy - Comprehensive set of limits are provided to prevent a single application, user and queue from monopolizing resources of the queue or the cluster.
    Priority Scheduling - This feature allows applications to be submitted and scheduled with different priorities. 
*******                 - Higher integer value indicates higher priority for an application. 
*******                 - Currently Application priority is supported only for FIFO ordering policy.

Configuration
- File is conf/yarn-site.xml
  yarn.resourcemanager.scheduler.class	=> org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler

Setting Queues
- etc/hadoop/capacity-scheduler.xml is the configuration file for the CapacityScheduler.
- The CapacityScheduler has a predefined queue called root. 
- All queues in the system are children of the root queue.

<property>
  <name>yarn.scheduler.capacity.root.queues</name>
  <value>a,b,c</value>
  <description>The queues at the this level (root is the root queue).
  </description>
</property>

<property>
  <name>yarn.scheduler.capacity.root.a.queues</name>                  ******** First child queue 'a' having to child queues 'a1', 'a2'
  <value>a1,a2</value>
  <description>The queues at the this level (root is the root queue).
  </description>
</property>

<property>
  <name>yarn.scheduler.capacity.root.b.queues</name>
  <value>b1,b2,b3</value>
  <description>The queues at the this level (root is the root queue).
  </description>
</property>

Queue Resource Allocation:
    yarn.scheduler.capacity.root.a.capacity=75.10
    yarn.scheduler.capacity.root.b.capacity=24.90

    yarn.scheduler.capacity.root.a.a1.capacity=60.8
    yarn.scheduler.capacity.root.a.a2.capacity=39.2

    yarn.scheduler.capacity.root.b.b1.capacity=48.2
    yarn.scheduler.capacity.root.b.b2.capacity=51.8
NOTE:
- Queue capacity in percentage (%) as a float (e.g. 12.5) OR as absolute resource queue minimum capacity. 
- The sum of capacities for all queues, at each level, must be equal to 100(root.a, root.b).
- However if absolute resource is configured, sum of absolute resources of child queues could be less than it’s parent absolute resource capacity.


FairScheduler
---------------------------------------------------------------------------------------------------------------------------------------------------------------
- Fair scheduling is a method of assigning RESOURCES to applications such that all apps get, on average, an equal share of resources over time. 
- By default, the Fair Scheduler bases scheduling fairness decisions only on memory.
- Hadoop NextGen is capable of scheduling multiple resource types. 

Allocation:
- When there is a single app running, that app uses the entire cluster. 
- When other apps are submitted, resources that free up are assigned to the new apps, so that each app eventually on gets roughly the same amount of resources.
- On other hand with default Hadoop scheduler(JobQueueTaskScheduler) and CapacityScheduler, which forms a queue of apps.

    
Reservation System
---------------------------------------------------------------------------------------------------------------------------------------------------------------
- The ReservationSystem of YARN provides ability to users to reserve resources over (and ahead of) time, 
- It helps to ensure that important production jobs will be run very predictably.
- ReservationSystem performs provides guarantees over absolute amounts of resources (instead of % of cluster size).
    
    
YARN Federation ( Ref link : https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/Federation.html)
---------------------------------------------------------------------------------------------------------------------------------------------------------------
Federation State-Store
  1. Sub-cluster Membership
    - Each sub-cluster YARN RMs continuously heartbeat to the state store to keep alive and publish their current capability/load information.
    - This information can be used by ROUTERS to select the best home sub-cluster.
  2. Application’s Home Sub-cluster
    - The sub-cluster on which the Application Master (AM) runs is called the Application’s “home sub-cluster”. 
    - The AM is not limited to resources from the home sub-cluster but can also request resources from other sub-clusters, referred to as secondary sub-clusters. 
    - The federated environment will be configured such that when an AM is placed on a sub-cluster, it should be able to find most of the resources on the home sub-cluster. 
    - Only in certain cases it should need to ask for resources from other sub-clusters.
    
Federation Policy Store
- The federation Policy Store is a logically separate store (while it might be backed by the same physical component)
- Contains information about how applications and resource requests are routed to different sub-clusters. 
- The current implementation provides policies like random/hashing/round-robin/priority etc for sub-cluster load, and request locality needs.

- In order to scale YARN beyond few thousands nodes, YARN supports the notion of Federation via the YARN Federation feature. 
- Federation allows to transparently wire together multiple YARN sub-clusters, and make them appear as a single massive cluster. 
- The federation system will stitch these sub-clusters together and make them appear as one large YARN cluster to the applications. 
- The applications running in this federated environment will see a single massive YARN cluster and will be able to schedule tasks on any node of the federated cluster. 
- Under the hood, the federation system will negotiate with sub-clusters resource managers and provide resources to the application. 

YARN Sub-cluster
- Each sub-cluster has its own YARN RM and compute nodes. 
- The exact size of the sub-cluster will be determined considering ease of deployment/maintenance, network or availability zones and general best practices etc.
- If the entire sub-cluster is compromised, external mechanisms will ensure that jobs are resubmitted in a separate sub-cluster
- Sub-cluster is also the SCALABILITY UNIT in a federated environment. We can scale out the federated environment by adding one or more sub-clusters.

Router
- YARN applications are submitted to one of the Routers, which applies a routing policy (obtained from the Policy Store), queries the State Store for the sub-cluster URL 
- Then it redirects the application submission request to the appropriate sub-cluster RM. 
- We call the sub-cluster where the job is started the “home sub-cluster”, and we call “secondary sub-clusters” all other sub-cluster a job is spanning on.

AMRMProxy
- The AMRMProxy is a key component to allow the application to scale and run across sub-clusters. 
- The AMRMProxy runs on all the NM machines and acts as a proxy to the YARN RM for the AMs by implementing ApplicationMasterProtocol.
****- Applications WILL NOT BE allowed to communicate with the sub-cluster RMs directly. 
- They are forced by the system to connect only to the AMRMProxy endpoint, which would provide transparent access to multiple YARN RMs.


Some useful commands
---------------------------------------------------------------------------------------------------------------------------------------------------------------
yarn application -status ApplicationID
yarn application -kill ApplicationID

